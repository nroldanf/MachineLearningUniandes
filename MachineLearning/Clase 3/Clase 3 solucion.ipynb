{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Universidad de Los Andes - Facultad de Economía** <br>\n",
    "**Machine Learning para Business Intelligence** <br>\n",
    "**_Paula Rodríguez, Juan S. Moreno, Mateo Dulce_**\n",
    "# Clase 3: Modelos de Regresión\n",
    "\n",
    "En esta clase se cubriran los modelos estándar de regresión:\n",
    "\n",
    "- Mínimos cuadrados ordinarios (Regresión Lineal)\n",
    "- Regresión polinomial\n",
    "- Regresión Logística\n",
    "- Linear Discriminant Analysis\n",
    "- Regularización L1 y L2\n",
    "- Árboles de regresión\n",
    "\n",
    "Además, para evaluar el desempeño de los modelos implementados, se estudiarán las métricas de evaluación:\n",
    "- R-cuadrado\n",
    "- MSE\n",
    "- MAPE\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/RodrigoLaraMolina/DPATTSrc/master/ML2.png\" alt=\"ML\" style=\"width: 500px;\" align=\"center\" frameborder=\"200\"/>\n",
    "\n",
    "_(Imagen tomada de [Medium - Different types of Machine learning and their types.](https://medium.com/deep-math-machine-learning-ai/different-types-of-machine-learning-and-their-types-34760b9128a2))_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introducción a Scikit-Learn\n",
    "\n",
    "Existen varias librerias de Python que proveen implementaciones solidas de multiples algoritmos de aprendizaje de máquinas. Una de las más conocidas es [Scikit-Learn](http://scikit-learn.org). Scikit-Learn se caracteriza por ser una API limpia, uniforme y optimizada, y por tener una documentación en línea muy útil y completa.Una ventaja de esta uniformidad es que una vez que comprende el uso básico y la sintaxis de Scikit-Learn para un tipo de modelo, cambiar a un nuevo modelo o algoritmo es muy sencillo.\n",
    "\n",
    "### Uso básico de la API\n",
    "\n",
    "Por lo general, los pasos a seguir para utilizar los modelos implementados en Scikit-Learn son:\n",
    "\n",
    "1. Seleccionar una clase de modelo importando la clase de estimador apropiada de Scikit-Learn.\n",
    "2. Seleccionar los hiper parámetros del modelo al instanciar la clase anterior con estos valores.\n",
    "3. Organizar los datos en una matriz de variables y un vector objetivo.\n",
    "4. Ajustar el modelo a los datos llamando al método `` fit () `` de la instancia del modelo.\n",
    "5. Evaluar el modelo en nuevos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mínimos Cuadrados Ordinarios\n",
    "### Regresión lineal univariada\n",
    "\n",
    "Los modelos de regresión lineal son un buen punto de partida para las tareas de regresión. Dichos modelos son populares porque pueden ajustarse muy rápidamente y son muy interpretables.\n",
    "\n",
    "Comenzaremos con la regresión lineal más familiar, un ajuste en línea recta a los datos.\n",
    "Un ajuste en línea recta es un modelo de la forma\n",
    "$$\n",
    "y = ax + b\n",
    "$$\n",
    "donde $a$ es la *pendiente*, y $b$ is el *intercepto*.\n",
    "\n",
    "Considere los siguientes datos, que están dispersos alrededor de un modelo lineal con una pendiente de 2 y una intersección de -5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar paquetes estandar\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar datos sintéticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para tener replicabilidad\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "# generamos puntos distribuidos de manera uniforme entre 0 y 10\n",
    "x = 10 * rng.rand(50,1)\n",
    "\n",
    "# generamos los valores de y usando un modelo lineal con pendiente 2\n",
    "# e intercepto -5 mas un ruido alatorio\n",
    "y = 2 * x[:,0] - 5 + rng.randn(50)\n",
    "\n",
    "# construit un grafico de puntos\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrar el mejor ajuste (usando Numpy)\n",
    "\n",
    "Escribiendo el modelo de forma matricial tenemos: $Y = X^T \\beta$.\n",
    "\n",
    "Queremos encontrar $\\hat \\beta$ que minimice el costo $(Y-\\hat Y)^2$ donde $\\hat Y = X^T \\hat \\beta$. <br>\n",
    "Esto es $\\hat \\beta = (X^TX)^{-1}X^TY$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolvamos el problema utilziando el paquete numpy\n",
    "import numpy as np\n",
    "\n",
    "print(type(x),type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la matriz que contiene una columna de 1's y la variable x\n",
    "X = np.matrix([np.ones(len(x),dtype=x.dtype),x.reshape(-1)])\n",
    "\n",
    "# np.matrix toma cada entrada como una fila.\n",
    "# trasponemos para tener observaciones x variables\n",
    "X=X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimación de beta\n",
    "beta_estimado = (np.linalg.inv(X.T*X)*X.T).dot(y)\n",
    "print(\"Pendiente del modelo: {}\".format(beta_estimado[0,1]))\n",
    "print(\"Intercepto del modelo: {}\".format(beta_estimado[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrar el mejor ajuste utilizando Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar LinearRegression de Scikit-Learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# crear un modelo lineal con intercepto\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "# ajustar el modelo a los datos\n",
    "model.fit(x, y)\n",
    "\n",
    "# graficar la funcion lineal estimada\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "# build plot\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit, color='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pendiente del modelo:    \", model.coef_[0])\n",
    "print(\"Intercepto del modelo:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de desempeño\n",
    "\n",
    "- $MSE = mean((Y-\\hat Y)^2)$ \n",
    "<br>\n",
    "<br>\n",
    "- $MAPE = mean(\\mid Y-\\hat Y \\mid)$ \n",
    "<br>\n",
    "<br>\n",
    "- $R^2 = \\dfrac{\\sigma^{2}_{XY}}{\\sigma^{2}_{X}\\sigma^{2}_{Y}}$ donde $\\sigma^{2}_{XY}$ es la covarianza de $X,Y$, $\\sigma^{2}_{X}$ la varianza de $X$ y $\\sigma^{2}_{Y}$ la varianza de $Y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizar la predicción dentro de muestra\n",
    "y_pred = model.coef_[0]*x + model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error cuadrático medio en el entrenamiento\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y,y_pred)\n",
    "print(\"El modelo de regresión lineal tiene un MSE = {0:.3f} dentro de muestra\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error cuadrático medio en el entrenamiento\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mape = mean_absolute_error(y,y_pred)\n",
    "print(\"El modelo de regresión lineal tiene un MAPE = {0:.3f} dentro de muestra\".format(mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error cuadrático medio en el entrenamiento\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y,y_pred)\n",
    "print(\"El modelo de regresión lineal tiene un R2 = {0:.3f} dentro de muestra\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen otros paquetes que contienen implementaciones de este mismo modelo. Por ejemplo pueden pribar con: <br>\n",
    "``statsmodels.formula.api.ols``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Lineal Múltiple\n",
    "\n",
    "El estimador ``LinearRegression`` también puede estimar modelos multivariables de la forma\n",
    "$$\n",
    "y = a_0 + a_1 x_1 + a_2 x_2 + \\cdots\n",
    "$$\n",
    "donde hay multiples valores para $x$.\n",
    "Geométricamente, esto es ajustar un plano a puntos en tres dimenciones o un hiper plano para dimensiones mayores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos datos con 5 variables distribuidas uniformemente entre 0 y 10\n",
    "# creamos la variable dependiente y como un modelo lineal de x\n",
    "rng = np.random.RandomState(1) # replicabilidad\n",
    "X = 10 * rng.rand(100, 2)\n",
    "y = 0.5 + np.dot(X, [1.5, -2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(211,projection='3d')\n",
    "\n",
    "ax.scatter(X.T[0],X.T[1], c='r', marker='o')\n",
    "\n",
    "# generar una grilla de x,y\n",
    "xx, yy = np.meshgrid(X.T[0],X.T[1])\n",
    "\n",
    "exog = pd.core.frame.DataFrame({'X': xx.ravel(), 'Y': yy.ravel()})\n",
    "out = model.predict(exog)\n",
    "ax.plot_surface(xx, yy,\n",
    "                out.reshape(xx.shape),\n",
    "                rstride=1,\n",
    "                cstride=1,\n",
    "                color='None',\n",
    "                alpha = 0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regresión polinomial\n",
    "\n",
    "Una manera de adaptar la regresión lineal a una relación no lineal entre las variabels es transformar los datos de acuerda a *funciones base*.\n",
    "La idea es tomar el modelo lineal multivariable:\n",
    "$$\n",
    "y = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + \\cdots\n",
    "$$\n",
    "y construir $x_1, x_2, x_3,$ del input unidimensional $x$.\n",
    "Tenemos entonces $x_n = f_n(x)$, donde $f_n()$ es una función que transforma nuestros datos.\n",
    "\n",
    "Por ejemplo, si $f_n(x) = x^n$, nuestro modelo se convierte en una regresión polinomial:\n",
    "$$\n",
    "y = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \\cdots\n",
    "$$\n",
    "Note que este modelo sigue siendo un *modelo lineal*. La linearidad se refiere al hecho de que los coeficientes $a_n$ son lineales entre ellos. Lo que hemos hecho es tomar la variable unidimensional $x$ y proyectarla en dimensiones mayores para que así el ajuste lineal pueda ajustar relaciones más complejas entre $x$ y $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de scikit learn que crea tranforma los datos\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "\n",
    "x = np.array([2, 3, 4])\n",
    "poly = PolynomialFeatures(3, include_bias=False)\n",
    "poly.fit_transform(x[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La manera más 'limpia' de realizar *feature engineering* en los modelos de ML es utilizando un pipeline. Creamos ahora un modelo polinomial de grado 7 usando ``make_pipeline`` y ``LinearRegression`` de Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el modelo polinomial de grado 7\n",
    "from sklearn.pipeline import make_pipeline\n",
    "poly_model = make_pipeline(PolynomialFeatures(7),\n",
    "                           LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1) #replicabilidad\n",
    "x = 10 * rng.rand(50) # creacion de x aleatorio entre 0 y 10\n",
    "y = np.sin(x) + 0.1 * rng.randn(50) # creacion de y como respuesta no lineal de x\n",
    "\n",
    "poly_model.fit(x[:, np.newaxis], y) #ajuste del modelo polinomial\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis]) #prediccion de y\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit, color='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar valores reales de y vs prediccion\n",
    "y_pred = poly_model.predict(x[:, np.newaxis])\n",
    "plt.scatter(y,y_pred)\n",
    "plt.xlabel(\"Real: $Y_i$\")\n",
    "plt.ylabel(\"Predicted: $\\hat{Y}_i$\")\n",
    "plt.title(\"Real vs Predicted: $Y_i$ vs $\\hat{Y}_i$\")\n",
    "plt.plot([-1,1.2],[-1,1.2],color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de grado con validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta y la siguiente sección estudiaremos el **Conjunto de datos de vivienda de Boston** que consiste en el precio de las casas en varios lugares de Boston. Junto con el precio, el conjunto de datos también proporciona información como la tasa de criminalidad (CRIM), áreas de negocios no minoristas en la ciudad (INDUS), la edad de las personas que poseen la casa (EDAD) y hay muchos otros atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.2.1. Boston house prices dataset\n",
    "\n",
    "# display an Inline Frame\n",
    "from IPython.display import IFrame \n",
    "IFrame('https://scikit-learn.org/stable/datasets/index.html#boston-dataset',\n",
    "       width=750, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the Boston data set in it's original format (dictionary)\n",
    "from sklearn.datasets import load_boston\n",
    "raw_data = load_boston()\n",
    "\n",
    "# Create a Pandas Data Frame with the Boston data\n",
    "boston = pd.DataFrame(raw_data.data)\n",
    "boston.columns = raw_data.feature_names\n",
    "boston['PRICE'] = raw_data.target\n",
    "display(boston.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos una función para el pipeline de la regresion polinomial\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos datos univariados del dataset de boston\n",
    "X = boston.LSTAT.values.reshape(-1, 1)\n",
    "y = boston.PRICE.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn; seaborn.set()  # plot formatting\n",
    "\n",
    "X_test = np.linspace(1, 40, 500)[:, None]\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "plt.scatter(X.ravel(), y, color='black',s=5)\n",
    "axis = plt.axis()\n",
    "for degree in [1, 3, 7]:\n",
    "    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\n",
    "    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\n",
    "#plt.xlim(-0.1, 1.0)\n",
    "#plt.ylim(-2, 12)\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "degree = np.arange(0, 21)\n",
    "train_score, val_score = validation_curve(PolynomialRegression(), X, y,\n",
    "                                          'polynomialfeatures__degree', degree, cv=7)\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validación con GridSearchCV\n",
    "\n",
    "<img src=\"img/fold-CV.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': np.arange(21),\n",
    "              'linearregression__fit_intercept': [True, False],\n",
    "              'linearregression__normalize': [True, False]}\n",
    "\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajustamos el GridSearchCV a los datos\n",
    "grid.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.scatter(X.ravel(), y, s=10)\n",
    "lim = plt.axis()\n",
    "y_test = model.fit(X, y).predict(X_test)\n",
    "plt.plot(X_test.ravel(), y_test, color='black');\n",
    "plt.axis(lim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regularización Ridge y Lasso\n",
    "\n",
    "La regresión de Ridge y Lasso son técnicas generalmente utilizadas para crear modelos que generealicen bien al tener una gran cantidad de variables. Aquí \"grande\" puede significar cualquiera de dos cosas:\n",
    "\n",
    "- Lo suficientemente grande como para mejorar la tendencia de un modelo a sobreajustar\n",
    "- Lo suficientemente grande como para causar desafíos computacionales\n",
    "\n",
    "Modelos de Regularización:\n",
    "\n",
    "1. **Ridge:** Realiza la regularización L2, es decir, agrega una penalización equivalente al cuadrado de la magnitud de los coeficientes\n",
    "2. **Lasso:** Realiza la regularización L1, es decir, agrega una penalización equivalente al valor absoluto de la magnitud de los coeficientes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(8)\n",
    "x = 10 * rng.rand(40)\n",
    "y = np.sin(x) + 0.2 * rng.randn(40)\n",
    "\n",
    "plt.scatter(x, y, s=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    x, y, test_size = 0.3, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class GaussianFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Uniformly spaced Gaussian features for one-dimensional input\"\"\"\n",
    "    \n",
    "    def __init__(self, N, width_factor=2.0):\n",
    "        self.N = N\n",
    "        self.width_factor = width_factor\n",
    "    \n",
    "    @staticmethod\n",
    "    def _gauss_basis(x, y, width, axis=None):\n",
    "        arg = (x - y) / width\n",
    "        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # create N centers spread along the data range\n",
    "        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n",
    "        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,\n",
    "                                 self.width_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(GaussianFeatures(18),\n",
    "                      LinearRegression())\n",
    "model.fit(X_train[:, np.newaxis], Y_train)\n",
    "\n",
    "plt.scatter(X_train, Y_train)\n",
    "plt.scatter(X_test, Y_test, color='red')\n",
    "plt.plot(np.linspace(0, 10, 1000), model.predict(xfit[:, np.newaxis]))\n",
    "plt.title(\"Regresión sin regularización\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = mean_squared_error(Y_train,model.predict(X_train[:, np.newaxis]))\n",
    "mse_test = mean_squared_error(Y_test,model.predict(X_test[:, np.newaxis]))\n",
    "print(\"El MSE en entrenamiento del modelo Rodge es: {0:.3f}\".format(mse_train))\n",
    "print(\"El MSE en prueba del modelo Rodge es: {0:.3f}\".format(mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rregresión Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = make_pipeline(GaussianFeatures(18), Ridge(alpha=0.01))\n",
    "ridge.fit(X_train[:, np.newaxis],Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, Y_train)\n",
    "plt.scatter(X_test, Y_test, color='red')\n",
    "plt.plot(np.linspace(0, 10, 1000), ridge.predict(xfit[:, np.newaxis]))\n",
    "plt.title(\"Regresión con regularización Ridge\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = mean_squared_error(Y_train,ridge.predict(X_train[:, np.newaxis]))\n",
    "mse_test = mean_squared_error(Y_test,ridge.predict(X_test[:, np.newaxis]))\n",
    "print(\"El MSE en entrenamiento del modelo Ridge es: {0:.3f}\".format(mse_train))\n",
    "print(\"El MSE en prueba del modelo Ridge es: {0:.3f}\".format(mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = make_pipeline(GaussianFeatures(18), Lasso(alpha=0.01))\n",
    "lasso.fit(X_train[:, np.newaxis],Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, Y_train)\n",
    "plt.scatter(X_test, Y_test, color='red')\n",
    "plt.plot(np.linspace(0, 10, 1000), lasso.predict(xfit[:, np.newaxis]))\n",
    "plt.title(\"Regresión con regularización Lasso\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = mean_squared_error(Y_train,lasso.predict(X_train[:, np.newaxis]))\n",
    "mse_test = mean_squared_error(Y_test,lasso.predict(X_test[:, np.newaxis]))\n",
    "print(\"El MSE en entrenamiento del modelo Lasso es: {0:.3f}\".format(mse_train))\n",
    "print(\"El MSE en prueba del modelo Lasso es: {0:.3f}\".format(mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Discriminant Analysis\n",
    "\n",
    "El _Linear Discriminant Analysis (LDA)_ es una técnica supervisada de reducción de dimensionalidad. El objetivo es proyectar un conjunto de datos en un espacio de dimensiones inferiores con buena capacidad de separación de clases para evitar el sobreajuste y también reducir los costos computacionales.\n",
    "\n",
    "<img src=\"img/lda1.png\" width=\"250\" class=\"center\"><img src=\"img/lda2.png\" width=\"250\" class=\"center\"><img src=\"img/lda3.png\" width=\"250\" class=\"center\"><img src=\"img/lda4.png\" width=\"250\" class=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine #importo datos\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis #importar LDA de scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "y = pd.Categorical.from_codes(wine.target, wine.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda = LinearDiscriminantAnalysis().fit_transform(X, y)\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.scatter(\n",
    "    X_lda[:,0],\n",
    "    X_lda[:,1],\n",
    "    c=pd.factorize(y)[0] + 1,\n",
    "    alpha=0.7,\n",
    "    edgecolors='b'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Árboles de regresión\n",
    "\n",
    "Cubriremos los siguientes temas:\n",
    "- Selección de hiper-parámetros.\n",
    "- Interpretación de los árboles.\n",
    "- Importancia de variables.\n",
    "- Tipos de ensambles: simulatáneos y secuenciales.\n",
    "- Regularización de árboles.\n",
    "- Evaluación de los modelos.\n",
    "\n",
    "Estudiaremos los siguientes tres modelos:\n",
    "- Árboles de regresión\n",
    "- Random Forest (Bosques aleatorios)\n",
    "- Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árboles de regresión\n",
    "\n",
    "Los árboles de decisión son formas extremadamente intuitivas para clasificar o etiquetar objetos: simplemente hace una serie de preguntas diseñadas para concentrarse en la regresión o clasificación.\n",
    "Por ejemplo, si desea construir un árbol de decisión para clasificar una casa, puede construir el que se muestra aquí:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/RodrigoLaraMolina/DPATTSrc/master/decision-tree-house.png\" alt=\"decision_tree\" style=\"width: 700px;\" align=\"center\" frameborder=\"200\"/>\n",
    "\n",
    "<img src=\"img/regression_trees.png\" width=\"800\" class=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tree from Scikit-Learn\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos las variables independientes y dependiente\n",
    "X = boston.drop('PRICE', axis = 1)\n",
    "Y = boston['PRICE']\n",
    "\n",
    "# importar función para separar dataset en entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separamos dataset con test del 30% de las observaciones\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size = 0.3, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el arbol de decisión\n",
    "tree_model = DecisionTreeRegressor(random_state=5)\n",
    "\n",
    "# ajustar el modelo a los datos\n",
    "tree_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualización del árbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar los paquetes para visualizar arboles de decision\n",
    "from graphviz import Source\n",
    "from IPython.display import Image\n",
    "\n",
    "# crear la visualizacion como png\n",
    "graph = Source(tree.export_graphviz(tree_model, out_file=None,\n",
    "                                     feature_names=X.columns))\n",
    "png_bytes = graph.pipe(format='png')\n",
    "with open('dtree_pipe.png','wb') as f:\n",
    "    f.write(png_bytes)\n",
    "\n",
    "# mostrar la imagen creada   \n",
    "from IPython.display import Image\n",
    "print('Double click to zoom')\n",
    "Image(png_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error de entranamiento y prueba\n",
    "pred_train = tree_model.predict(X_train)\n",
    "pred_test= tree_model.predict(X_test)\n",
    "mse_train = mean_squared_error(Y_train,pred_train)\n",
    "mse_test = mean_squared_error(Y_test, pred_test)\n",
    "print(\"El mse de entrenamiento es: {0:.3f}\".format(mse_train))\n",
    "print(\"El mse de prueba es: {0:.3f}\".format(mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HAY OVERFITTING!!!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularización de árboles (_podar el árbol_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': list(range(2,15)), \n",
    "              'max_leaf_nodes': [5,10,15,20,None]}\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeRegressor(random_state=5), param_grid, cv=5)\n",
    "grid.fit(X_train, Y_train)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear la visualizacion como png\n",
    "graph = Source(tree.export_graphviz(best_tree, out_file=None,\n",
    "                                     feature_names=X.columns))\n",
    "png_bytes = graph.pipe(format='png')\n",
    "with open('best_tree.png','wb') as f:\n",
    "    f.write(png_bytes)\n",
    "\n",
    "# mostrar la imagen creada   \n",
    "from IPython.display import Image\n",
    "print('Double click to zoom')\n",
    "Image(png_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error de entranamiento y prueba\n",
    "pred_train = best_tree.predict(X_train)\n",
    "pred_test= best_tree.predict(X_test)\n",
    "mse_train = mean_squared_error(Y_train,pred_train)\n",
    "mse_test = mean_squared_error(Y_test, pred_test)\n",
    "print(\"El mse de entrenamiento para el mejor árbol es: {0:.3f}\".format(mse_train))\n",
    "print(\"El mse de prueba para el mejor árbol es: {0:.3f}\".format(mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables más importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un dataframe con las importancias\n",
    "importancia_arbol=pd.DataFrame(best_tree.feature_importances_, index=X.columns, columns=['Importancia'])\n",
    "importancia_arbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest y Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor # Importo la función para estimar RandomForest\n",
    "from sklearn.ensemble import GradientBoostingRegressor # Importo la función para estimar el boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino el RandomForest\n",
    "rf = RandomForestRegressor(random_state=23)\n",
    "\n",
    "# ajusto el modelos\n",
    "rf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino el GBM\n",
    "gbm = GradientBoostingRegressor(random_state=23)\n",
    "\n",
    "# ajusto el modelos\n",
    "gbm.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error de entranamiento y prueba\n",
    "pred_train_rf = rf.predict(X_train)\n",
    "pred_test_rf= rf.predict(X_test)\n",
    "\n",
    "pred_train_gbm = gbm.predict(X_train)\n",
    "pred_test_gbm= gbm.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"El mse de entrenamiento para el RF es: {0:.3f}\".format(mean_squared_error(Y_train,pred_train_rf)))\n",
    "print(\"El mse de prueba para el RF es: {0:.3f}\".format(mean_squared_error(Y_test,pred_test_rf)))\n",
    "print(\" \")\n",
    "print(\"El mse de entrenamiento para el GBM es: {0:.3f}\".format(mean_squared_error(Y_train,pred_train_gbm)))\n",
    "print(\"El mse de prueba para el GBM es: {0:.3f}\".format(mean_squared_error(Y_test,pred_test_gbm)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Ejercicio para la casa:** Utilice ``GridSearchCV`` para seleccionar los parámetros que mejor ajusten los modelos de Random Forest y Gradient Boosting._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen Rápido: Qué hemos hecho hasta ahora?\n",
    "_Imagen tomada de [Dataconomy: INFOGRAPHIC: A BEGINNER’S GUIDE TO MACHINE LEARNING ALGORITHMS](https://dataconomy.com/2017/03/beginners-guide-machine-learning/)_\n",
    "\n",
    "<img src=\"img/resumen.png\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regresión Logística\n",
    "\n",
    "El objetivo de esta sección es usar datos del orden financiero para predecir la probabilidad de que una persona deje de pagar sus deudas (incurra en default) dadas sus características observadas. Los datos fueron obtenidos del repositorio de machine learning de la UCI y corresponden a información crediticia de un importante banco en Taiwan.\n",
    "\n",
    "Las variables corresponden a: \n",
    "\n",
    "- ID: Variable de identificación del cliente.\n",
    "- LIMIT_BAL: Valor del crédito en dólares NT.\n",
    "- SEX: Género de la persona (1= hombre, 2= mujer).\n",
    "- EDUCATION: Nivel educativo (1= posgrado, 2= pregrado, 3=secundaria, 4=otros)\n",
    "- MARRIAGE: Estado civil (1= casado, 2=soltero, 3= otro)\n",
    "- AGE= Edad en años.\n",
    "- PAY_0-PAY_6 = Historial de pagos pasados (de abril a septiembre). La escala de los estados son: -1= a tiempo, 1= pago con retraso de un mes, 2=  pago con retraso de dos meses, .....9= pago con retraso de nueve meses o más.\n",
    "- BILL_AMT1- BILL_AMT6=  Valor a pagar (factura) en dólar NT (de abril a septiembre).\n",
    "- PAY_AMT1-PAY_AMT6: Valor efectivamente pagado en dólar NT (de abril a septiembre)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Procesamiento de datos\n",
    "import os  # Cambiar directorio de trabajo\n",
    "import numpy as np # Arreglos númericos\n",
    "from matplotlib import pyplot as plt # Gráficos\n",
    "\n",
    "from sklearn.compose import ColumnTransformer  # Utilidades de  Preprocesamiento\n",
    "from sklearn.pipeline import Pipeline          # Utilidades de  Preprocesamiento\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder #Estandarización y dummies\n",
    "from sklearn.linear_model import LogisticRegression  # Regresión logística\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV # Partición de datos, validación cruzada\n",
    "import  sklearn.metrics as metrics  # Métricas de desempeño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar datos\n",
    "dt= pd.read_csv(\"default.csv\")\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionar variables categóricas\n",
    "catvars= list([\"SEX\",\"EDUCATION\", \"MARRIAGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"]) \n",
    "\n",
    "# Convertir variables categóricas\n",
    "for col in catvars:\n",
    "    dt[col] = dt[col].astype('category')\n",
    "\n",
    "# Mostrar tipos de variables\n",
    "dt.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar características y marca\n",
    "X = dt.drop(['default'], axis=1)\n",
    "y = dt['default']\n",
    "\n",
    "#convertir variables categoricas en dummies\n",
    "X = pd.get_dummies(X, columns=catvars)\n",
    "\n",
    "# Particionar los datos  70% entrenamiento, 30% prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarizar train y test\n",
    "# Seleccionar variables númericas\n",
    "numvars=X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "X_train.loc[:, numvars] = StandardScaler().fit_transform(X_train.loc[:, numvars])\n",
    "X_test.loc[:, numvars] = StandardScaler().fit_transform(X_test.loc[:, numvars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(888)\n",
    "\n",
    "# Regresión logística\n",
    "logit = LogisticRegression(solver=\"lbfgs\", max_iter=2000, C=1000000)\n",
    "\n",
    "# Ajustar Modelo\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "pred_train = logit.predict_proba(X_train)\n",
    "pred_test = logit.predict_proba(X_test)\n",
    "\n",
    "# Calcular MSE\n",
    "mse_train = mean_squared_error(y_train.values, pred_train)\n",
    "mse_test = mean_squared_error(y_test.values, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
