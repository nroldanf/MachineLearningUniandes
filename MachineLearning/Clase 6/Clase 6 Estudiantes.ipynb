{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Universidad de Los Andes - Facultad de Economía** <br>\n",
    "**Machine Learning para Business Intelligence** <br>\n",
    "**_Paula Rodríguez, Juan S. Moreno, Mateo Dulce_**\n",
    "# Clase 6: Series de Tiempo\n",
    "\n",
    "**Lo que cubriremos esta clase:**\n",
    "1. Introducción: Definiciones y métricas de calidad\n",
    "2. Mover, suavizar y evaluar series de tiempo\n",
    "3. Pronósticos:\n",
    "    - Validación cruzada para series de tiempo\n",
    "    - Modelos con enfoque econométrico\n",
    "\n",
    "**Recursos adicionales:**\n",
    "- Recurso teórico: [_Statistical forecasting: notes on regression and time series analysis, Robert Nau_](https://people.duke.edu/~rnau/411home.htm)\n",
    "- Recurso práctico (programación): [Time Series Analysis (TSA) in Python - Linear Models to GARCH](http://www.blackarbs.com/blog/time-series-analysis-in-python-linear-models-to-garch/11/1/2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción: Definiciones y métricas de calidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Una serie de tiempo es una serie de puntos de datos indexados (o listados, o graficados) en orden de tiempo._**\n",
    "\n",
    "Importemos algunas librerías. Primero, necesitaremos la librería [statsmodels] (http://statsmodels.sourceforge.net/stable/), que tiene muchas funciones de modelación estadística, incluidas series de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectores y matrices\n",
    "# tablas y manipulacion de datos\n",
    "# graficos\n",
    "\n",
    "\n",
    "# manejo de fechas\n",
    "# para minimizar funciones\n",
    "# estadictica y econometria\n",
    "\n",
    "\n",
    "# alguna funciones utiles\n",
    "\n",
    "\n",
    "# modo `no molestar`\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ejemplo, miremos datos reales de juegos móviles. Específicamente, analizaremos los anuncios vistos por hora y el gasto de dinero en el juego por día:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargar datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafiquemos la serie de tiempo de anuncios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafiquemos la serie de tiempo de gastos diarios en juegos \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas para pronósticos\n",
    "\n",
    "Antes de comenzar a pronosticar, veamos cómo medir la calidad de nuestras predicciones.\n",
    "\n",
    "- [R squared](http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination): coeficiente de determinación (en econometría, esto puede interpretarse como el porcentaje de varianza explicado por el modelo), $(-\\infty, 1]$\n",
    "\n",
    "$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$ \n",
    "\n",
    "```python\n",
    "sklearn.metrics.r2_score\n",
    "```\n",
    "---\n",
    "- [Mean Absolute Error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error): esta es una métrica interpretable porque tiene la misma unidad de medida que la serie inicial, $[0, +\\infty)$\n",
    "\n",
    "$MAE = \\frac{\\sum\\limits_{i=1}^{n} |y_i - \\hat{y}_i|}{n}$ \n",
    "\n",
    "```python\n",
    "sklearn.metrics.mean_absolute_error\n",
    "```\n",
    "---\n",
    "- [Median Absolute Error](http://scikit-learn.org/stable/modules/model_evaluation.html#median-absolute-error): nuevamente, una métrica interpretable que es particularmente interesante porque es robusta para los valores atípicos, $[0, +\\infty)$\n",
    "\n",
    "$MedAE = median(|y_1 - \\hat{y}_1|, ... , |y_n - \\hat{y}_n|)$\n",
    "\n",
    "```python\n",
    "sklearn.metrics.median_absolute_error\n",
    "```\n",
    "---\n",
    "- [Mean Squared Error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error): la métrica más utilizada que otorga una penalización más alta a los errores grandes y viceversa, $[0, +\\infty)$\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "```python\n",
    "sklearn.metrics.mean_squared_error\n",
    "```\n",
    "---\n",
    "- [Mean Squared Logarithmic Error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-logarithmic-error): prácticamente, esto es lo mismo que MSE, pero tomamos el logaritmo de la serie. Como resultado, también damos más peso a los pequeños errores. Esto generalmente se usa cuando los datos tienen tendencias exponenciales, $[0, +\\infty)$\n",
    "\n",
    "$MSLE = \\frac{1}{n}\\sum\\limits_{i=1}^{n} (log(1+y_i) - log(1+\\hat{y}_i))^2$\n",
    "\n",
    "```python\n",
    "sklearn.metrics.mean_squared_log_error\n",
    "```\n",
    "---\n",
    "- Mean Absolute Percentage Error: Esto es lo mismo que MAE, pero se calcula como un porcentaje. $[0, +\\infty)$\n",
    "\n",
    "$MAPE = \\frac{100}{n}\\sum\\limits_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{y_i}$ \n",
    "\n",
    "```python\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mover, suavizar y evaluar series de tiempo\n",
    "\n",
    "Comencemos con la idea de que \"mañana será lo mismo que hoy\". Sin embargo, en lugar de un modelo como $ \\hat {y} _ {t} = y_ {t-1} $, asumiremos que el valor futuro de nuestra variable depende del promedio de sus valores anteriores de $ k $. Por lo tanto, utilizaremos el **promedio móvil**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizando un promedio movil de 24 horas, cuántos anuncios se verán en 2017-09-22 00:00:00?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desafortunadamente, no podemos hacer predicciones en el futuro: para obtener el valor para el siguiente paso, necesitamos que se observen los valores anteriores. Pero la media móvil tiene otro caso de uso: **suavizar la serie de tiempo original para identificar tendencias**. </br>\n",
    "\n",
    "Pandas tiene una implementación disponible con [`DataFrame.rolling (window) .mean ()`] (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html). Cuanto más ancha es la ventana, más suave es la tendencia. En el caso de datos muy ruidosos este procedimiento puede ayudar a detectar patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usar .rolling para obtener promedio en ventana de 12 horas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafiquemos la serie de tiempo obtenida por promedio movil de 8 horas y la serie de tiempo original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definamos una funcion para graficar y poder cambiar la vetana de tiempo\n",
    "# grafiquemos también los intervalos de confianza para la suavizavión por promedio movil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veamos las tendencias diarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar promedio movil con ventana de 2 e intervalos de confianza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veamos la tendencia semanal del gasto en los juegos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**no capturó la estacionalidad mensual en nuestros datos y marcó casi todos los picos de 30 días como anomalías!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Promedio ponderado:** $\\hat{y}_{t} = \\displaystyle\\sum^{k}_{n=1} \\omega_n y_{t+1-n}$\n",
    "</br>\n",
    "</br>\n",
    "- **Suavizamiento exponencial:** $\\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1} \\quad ; \\quad \\hat y_{0} = y_{0} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizar una suavizacion exponencial con alpha = 0.1\n",
    "#el primer valor es el mismo que el de la serie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar suavizamiento exponencial con alpha = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suavizamiento_exponencial(series, alpha):\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grafSuavExponencial(series, alphas):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar suav exponencial con alpha de 0.3 y 0.05 para ads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar suav exponencial con alpha de 0.3 y 0.05 para currency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suavizamiento exponencial doble\n",
    "\n",
    "- $\\ell_x = \\alpha y_x + (1-\\alpha)(\\ell_{x-1} + b_{x-1}) \\quad \\quad \\quad$        _Describe el intercepto_ \n",
    "\n",
    "- $b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1} \\quad \\quad \\quad$ _Describe la tendencia_\n",
    "\n",
    "- $\\hat{y}_{x+1} = \\ell_x + b_x \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad$ _Suma de los valores del modelo de la intersección y la tendencia_ \n",
    "\n",
    "\n",
    "### Suavizamiento exponencial triple (Holt-Winters)\n",
    "La idea es agregar un tercer componente: estacionalidad\n",
    "\n",
    "``statsmodels.tsa.holtwinters``\n",
    "\n",
    "- $\\ell_x = \\alpha(y_x - s_{x-L}) + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$\n",
    "\n",
    "- $b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$\n",
    "\n",
    "- $s_x = \\gamma(y_x - \\ell_x) + (1-\\gamma)s_{x-L}$\n",
    "\n",
    "- $\\hat{y}_{x+m} = \\ell_x + mb_x + s_{x-L+1+(m-1)modL}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pronósticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Validación cruzada para series de tiempo\n",
    "\n",
    "1. Entrenamos nuestro modelo en un pequeño segmento de la serie temporal desde el principio hasta $t$\n",
    "2. Hacemos predicciones para los próximos pasos de $t + n$ y calculamos el error\n",
    "3. Ampliamos nuestra muestra de entrenamiento a $t + n$\n",
    "4. Hacemos predicciones desde $ t + n $ hasta $ t + 2 * n $\n",
    "5. Continuamos moviendo nuestro segmento de prueba de la serie de tiempo hasta llegar a la última observación disponible\n",
    "\n",
    "Como resultado, tenemos tantos pliegues como $ n $ encajarán entre la muestra de entrenamiento inicial y la última observación.\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/f5c/7cd/b39/f5c7cdb39ccd4ba68378ca232d20d864.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Modelos con enfoque econométrico\n",
    "\n",
    "#### Estacionalidad\n",
    "\n",
    "\n",
    "Un proceso es **estacionario** si no cambia sus propiedades estadísticas con el tiempo, es decir, su media, varianza y covarianza. (La constancia de la varianza se denomina **homocedasticidad**). La función de covarianza no depende del tiempo; solo debe depender de la distancia entre observaciones. \n",
    "\n",
    "**¿Por qué las siguientes series de tiempo son o no estacionarias?**\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/20c/9d8/a63/20c9d8a633ec436f91dccd4aedcc6940.png\"/>\n",
    "\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/b88/eec/a67/b88eeca676d642449cab135273fd5a95.png\"/>\n",
    "\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/2f6/1ee/cb2/2f61eecb20714352840748b826e38680.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estacionalidad, correlacion y autocorrelación parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prueba de raíz unitaria de Dickey-Fuller aumentada**\n",
    "\n",
    "Si se tiene el proceso estocástico en tiempo discreto $\\lbrace y_t; t=1,...,\\infty \\rbrace$ y suponemos que se puede escribir como un proceso autorregresivo de orden $p$:\n",
    "\n",
    "$$y_t = a_1 y_{t-1} + a_2 y_{t-2} + ...+ a_p y_{t-p} + \\epsilon_t$$\n",
    "\n",
    "Entonces la **ecuación característica** del proceso es:\n",
    "\n",
    "$$x^p - x^{p-1}a_1 - x^{p-2}a_2 - ...- a_p = 0.$$\n",
    "\n",
    "Un proceso estocástico lineal tiene una **raíz unitaria** si el valor de la raíz de la ecuación característica del proceso es igual a 1, por lo tanto tal proceso es no estacionario.\n",
    "\n",
    "La prueba de **Dickey-Fuller** aumentada se puede usar para probar una raíz unitaria en un proceso univariante en presencia de correlación serial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficar la serie de tiempo de ads e imprimir el p-valor del test de Dickey-Fuller\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Autocorrelación serial (ACF**): En el rezago k, ésta es la correlación entre los valores de la serie que están separados por k intervalos.\n",
    "\n",
    "- **Autocorrelación parcial (PACF)**. En el rezago k, ésta es la correlación entre los valores de la serie que están separados por k intervalos, teniendo en cuenta los valores de los intervalos entre ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplot(y, lags=None, figsize=(12, 7)):\n",
    "\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "        \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    layout = (2, 2)\n",
    "    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "    acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "    pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "\n",
    "    y.plot(ax=ts_ax)\n",
    "    p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "    ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n",
    "    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficos con lags=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estudiemos la serie de tiempo quitando el componente de periodiciad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estuddiemos la serie de tiempo de la primera diferencia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos ARIMA\n",
    "\n",
    "Explicaremos este modelo construyendo letra por letra. $ SARIMA (p, d, q) (P, D, Q, s) $, modelo de media móvil de autorregresión estacional (Seasonal Autoregression Moving Average):\n",
    "\n",
    "- $AR(p)$ - modelo de autorregresión, es decir, regresión de la serie temporal sobre sí misma. La suposición básica es que los valores de la serie actual dependen de sus valores anteriores con algún rezago (o varios rezagos). El rezago máximo en el modelo se conoce como $p$. Para determinar el $p$ inicial, debe mirar el gráfico PACF y encontrar el mayor rezago significativo después del cual **la mayoría** de los otros rezagos se vuelven insignificantes.\n",
    "- $MA(q)$ - modelo de media móvil. Sin entrar en demasiados detalles, esto modela el error de la serie de tiempo, nuevamente asumiendo que el error actual depende del anterior con algún rezago, lo que se conoce como $q$. El valor inicial se puede encontrar en el gráfico ACF con la misma lógica que antes.\n",
    "\n",
    "$$AR(p) + MA(q) = ARMA(p, q)$$\n",
    "\n",
    "- $I(d)$ - orden de integración. Este es simplemente el número de diferencias no estacionales necesarias para que la serie sea estacionaria. En nuestro caso, es solo 1 porque usamos las primeras diferencias.\n",
    "\n",
    "- $S(s)$ - esto es responsable de la estacionalidad e iguala la duración del período de la temporada de la serie\n",
    "\n",
    "Con esto tenemos los parámetros: $(P, D, Q)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escogencia de algunos valores iniciales\n",
    "\n",
    "\n",
    "# creamos una lista con todas las posibles combinaciones de parametros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeSARIMA(parameters_list, d, D, s):\n",
    "    \n",
    "    results = []\n",
    "    best_aic = float(\"inf\")\n",
    "\n",
    "    for param in tqdm_notebook(parameters_list):\n",
    "        # agregamos un try porque alguna combinaciones de parametros pueden no converger\n",
    "        try:\n",
    "            model=sm.tsa.statespace.SARIMAX(ads.Ads, order=(param[0], d, param[1]), \n",
    "                                            seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)\n",
    "        except:\n",
    "            continue\n",
    "        aic = model.aic\n",
    "        # guardamos el mejor modelo, su AIC y sus parametros\n",
    "        if aic < best_aic:\n",
    "            best_model = model\n",
    "            best_aic = aic\n",
    "            best_param = param\n",
    "        results.append([param, model.aic])\n",
    "\n",
    "    result_table = pd.DataFrame(results)\n",
    "    result_table.columns = ['parameters', 'aic']\n",
    "\n",
    "    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    return result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tabla_resultados = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleccionar los parametros que tienen un menos AIC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSARIMA(series, model, n_steps):\n",
    "\n",
    "    # adding model values\n",
    "    data = series.copy()\n",
    "    data.columns = ['actual']\n",
    "    data['arima_model'] = model.fittedvalues\n",
    "    # making a shift on s+d steps, because these values were unobserved by the model\n",
    "    # due to the differentiating\n",
    "    data['arima_model'][:s+d] = np.NaN\n",
    "    \n",
    "    # forecasting on n_steps forward \n",
    "    forecast = model.predict(start = data.shape[0], end = data.shape[0]+n_steps)\n",
    "    forecast = data.arima_model.append(forecast)\n",
    "    # calculate error, again having shifted on s+d steps from the beginning\n",
    "    error = mean_absolute_percentage_error(data['actual'][s+d:], data['arima_model'][s+d:])\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n",
    "    plt.plot(forecast, color='r', label=\"model\")\n",
    "    plt.axvspan(data.index[-1], forecast.index[-1], alpha=0.5, color='lightgrey')\n",
    "    plt.plot(data.actual, label=\"actual\")\n",
    "    plt.legend()\n",
    "    plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSARIMA(ads, best_model, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
