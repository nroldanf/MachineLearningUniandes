{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 7: Minería de texto\n",
    "En esta clase vamos a ver el tratamiento básico de texto, expresiones regulares, un modelo de tópicos __Latent Dirichlet Allocation (LDA)__.\n",
    "\n",
    "Cubriremos los siguientes temas:\n",
    "* Tratamiento de textos\n",
    "* Preprocesamiento de textos\n",
    "* WordClouds\n",
    "* Document-Term Matrix\n",
    "* LDA\n",
    "* Interpretación de resultados\n",
    "    * Palabras más importantes\n",
    "    * Distribución de textos por tópicos\n",
    "* Selección de modelo\n",
    "* Visualización (LDAvis)\n",
    "\n",
    "Para estudiar LDA, a parte del paper original, les recomendamos el siguiente video: https://www.youtube.com/watch?v=3mHy4OSyRf0&t=6s\n",
    "\n",
    "La base de datos que trabajaremos será una muestra de noticias de [Reuters](https://www.reuters.com/) obtenida a través de webscrapping por Germán González.\n",
    "\n",
    "1. ¿Qué significa que un problema sea de análisis no supervisado?\n",
    "2. ¿Cómo evalúo uno de estos modelos?\n",
    "3. ¿Que son datos no estructurados?\n",
    "4. ¿Cómo entiende un computador los datos no estructurados?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re # El paquete para tratar texto. Expresiones regulares\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Vectorizador de palabras y DTM\n",
    "from sklearn.decomposition import LatentDirichletAllocation # Modelo de LDA\n",
    "from scipy.sparse import csr_matrix # Para tratar Sparse Matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('reuters.csv') # CArgo los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Explore el encabezado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Exploro una noticia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento\n",
    "* Tokenizar: Separar el texto en párrafos, frases, etc...\n",
    "* Limpieza: Minúsculas, quito puntuación, remuevo palabras de 3 caracteres.\n",
    "* Stopwords\n",
    "* Lematizar: cambio de tiempos verbales\n",
    "* Stemmed: enviar palabras a sus raíces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza básica\n",
    "Para ver más sobre expresiones regulares vea: https://www.w3schools.com/python/python_regex.asp\n",
    "\n",
    "Puede practicar en: https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Busco las palabras que comienzan por f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "Extraiga de una noticia cualquiera lo siguiente:\n",
    "1. Los números. (piense en los decimales)\n",
    "2. Los número que correspondan a porcentajes (vea la palabra percent).\n",
    "4. Todas la palabras que comiencen por Mayúscula.\n",
    "5. Todas la palabras que comiencen por Mayúscula pero el resto de caracteres estén en minúscula.\n",
    "5. Extraiga todas las palabras que inicien por `r` (mayúscula o minúscula)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esriba la solución aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limpieza.\n",
    "### Se usa los métodos de pandas que provienen de .str\n",
    "\n",
    "# Envío a minúsculas\n",
    "# Borro Puntuaciones\n",
    "# Quito números\n",
    "# Quito la palabra reuters\n",
    "# Quito los dobles espacios\n",
    "# Convierto minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a ver la misma noticia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora construiremos la matriz término-documento\n",
    " # máximo tamaño de vocabulario\n",
    "# Al igual que un modelo, defino el objeto que construirá la matriz\n",
    "# Aplico el objeto a un conjunto de textos\n",
    "# Veo el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploremos los stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Vuelvo de sparse a densa para explorarla\n",
    "#Veo las primeras 5 filas\n",
    "# Veo las dimensiones, a qué corresponden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploramos la matriz término-documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Cuántas veces aparece year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Que tal si estudiamos las frecuencias de las palabras?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafique en barras las frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud #importo la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construyo el generador de la nube\n",
    " # Genero la nube\n",
    " # Despliego la imagen de la nube\n",
    "# Para ver las gamas de colores vea: https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n",
    "# Para ver más parámetros de la nube de palabras: https://amueller.github.io/word_cloud/auto_examples/index.html#example-gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Cuántos tópicos deseo\n",
    " # Construyo el objeto que es el modelo\n",
    " # Estimo el LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De que tma~no es el resultado?\n",
    " # Exploremos el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construyo la función que me ayuda a ver las palabras más importantes de cada tópico\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names() # extraigo las palabras del modelo\n",
    "    for topic_idx, topic in enumerate(model.components_): # Hago un for que recorre por filas, recuerde que cada fila es un tópico, cada columna una palabra\n",
    "        print(\"\\nTopic #%d:\" % topic_idx) # Imprima el número de tópico\n",
    "        print(\", \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]])) #Muestre las n palabras más importantes por orden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Veo las 15 palabras más importantes de cada tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como se ven los documentos?\n",
    " # transformo la matrix de término-documento en tópico-documento\n",
    " # Qué indican las dimensiones?\n",
    " # Nombres de filas\n",
    " # Nombres de columnas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploremos la salida desde el punto de vista de documentos\n",
    " # Porque las filas suman 1?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cómo se distribuye el documento promedio?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creemos la pertenencia al tópicos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos el histograma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de modelo\n",
    "Al ser análisis no supervisado no es nada fácil escoger el mejor modelo, y es aún más retador cuando es texto. Tenemos una aproximación, la máxima verosimilitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Juguemos con un hiper parámetro\n",
    "likelihood=[]\n",
    "values=[i for i in range(2,31,2)]\n",
    "for i in values:\n",
    "    modelo = LatentDirichletAllocation(n_components=i, max_iter=10,doc_topic_prior=0.1, topic_word_prior=0.1, n_jobs=-1,random_state=23) # Construyo el objeto que es el modelo\n",
    "    modelo.fit(tf)\n",
    "    likelihood.append(modelo.score(tf))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(values, likelihood)\n",
    "plt.xlabel('Número de tópicos')\n",
    "plt.ylabel('log-likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización del LDA\n",
    "LDAvis es un paquete para la visualización para interpretar más fácilmente el LDA. Vea el paper [aquí](https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf), en este agregan un nuevo parámetro para la interpretació, $\\lambda \\in [0,1]$, el cual pondera la importancia de una palabra dentro del tópico por la unicidad de la palabra a lo largo de los tópicos. \n",
    "* $\\lambda\\rightarrow1$: este es el caso original, permite que la palabra sea repetida a lo largo de los tópicos.\n",
    "* $\\lambda\\rightarrow0$: este caso cambia el rqanking de importancia, dándole mayor importancia a las palabras que sean únicas del tópico, es decir que no aparezcan casi en los demás.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis # Paquete que crea la visualización\n",
    "from pyLDAvis import sklearn as sklearnlda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Preparo el modelo y sus resultados para la visualización\n",
    "# Guardo la visualización como html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Lo visualizo dentro del notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
